import React, { useState, useEffect, useRef } from 'react';
import { AppState, Language, LLMResponse } from './types';
import { t } from './i18n';
import { 
  SpeechRecognitionService, 
  isSpeechRecognitionSupported, 
  speak, 
  loadVoices 
} from './utils/speech';
import { callLLM } from './utils/llm';
import VoiceOrb from './components/VoiceOrb';
import PictureBoard from './components/PictureBoard';
import './App.css';

function App() {
  const [state, setState] = useState<AppState>('IDLE');
  const [language, setLanguage] = useState<Language>('zh');
  const [currentSentence, setCurrentSentence] = useState({ zh: '', en: '' });
  const [realtimeTranscript, setRealtimeTranscript] = useState(''); // å®æ—¶è¯†åˆ«æ–‡å­—
  const [audioLevel, setAudioLevel] = useState(0); // éŸ³é‡çº§åˆ« 0-1
  const [isSpeaking, setIsSpeaking] = useState(false); // æ˜¯å¦æ£€æµ‹åˆ°è¯´è¯ï¼ˆç”¨äºå¼ºåé¦ˆï¼‰
  
  const recognitionRef = useRef<SpeechRecognitionService | null>(null);
  const isRecordingRef = useRef(false);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const hadVoiceRef = useRef<boolean>(false);

  // åˆå§‹åŒ–
  useEffect(() => {
    loadVoices();
    if (isSpeechRecognitionSupported()) {
      recognitionRef.current = new SpeechRecognitionService(language, setRealtimeTranscript);
    } else {
      // æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«ï¼Œæ˜¾ç¤ºé”™è¯¯
      console.error('Browser does not support speech recognition');
    }
    
    // æ¸…ç†
    return () => {
      stopAudioMonitoring();
    };
  }, []);

  // æ›´æ–°è¯­éŸ³è¯†åˆ«è¯­è¨€
  useEffect(() => {
    if (recognitionRef.current) {
      recognitionRef.current.setLanguage(language);
    }
  }, [language]);

  // éŸ³é‡ç›‘æµ‹
  const startAudioMonitoring = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
      const audioContext = new AudioContext();
      const analyser = audioContext.createAnalyser();
      const microphone = audioContext.createMediaStreamSource(stream);
      
      analyser.fftSize = 256;
      microphone.connect(analyser);
      
      audioContextRef.current = audioContext;
      analyserRef.current = analyser;
      
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      let speakingState = false;
      
      const timeDomainArray = new Uint8Array(analyser.frequencyBinCount);

      const updateLevel = () => {
        if (!analyserRef.current || state !== 'LISTENING') {
          return;
        }
        
        // é¢‘åŸŸèƒ½é‡
        analyser.getByteFrequencyData(dataArray);
        const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;
        const freqLevel = average / 80; // é¢‘åŸŸç²—ç•¥å½’ä¸€åŒ–

        // æ—¶åŸŸ RMSï¼ˆå¯¹å®‰é™è®¾å¤‡æ›´ç¨³ï¼‰
        analyser.getByteTimeDomainData(timeDomainArray);
        let sumSquares = 0;
        for (let i = 0; i < timeDomainArray.length; i++) {
          const v = (timeDomainArray[i] - 128) / 128; // -1..1
          sumSquares += v * v;
        }
        const rms = Math.sqrt(sumSquares / timeDomainArray.length); // 0..1
        const timeLevel = rms * 3; // æ”¾å¤§ç³»æ•°æé«˜çµæ•åº¦

        const normalizedLevel = Math.min(Math.max(freqLevel, timeLevel), 1);
        
        setAudioLevel(normalizedLevel);
        console.log('Audio level:', normalizedLevel.toFixed(3), 'freqAvg:', average.toFixed(1), 'rms:', rms.toFixed(3));
        
        // è¯´è¯é˜ˆå€¼ï¼ˆå¸¦è¿Ÿæ»ï¼Œå‡å°‘é—ªçƒï¼‰- å†é™ä½ä¸€æ¡£
        const HIGH = 0.05;
        const LOW = 0.03;
        if (!speakingState && normalizedLevel > HIGH) {
          speakingState = true;
          setIsSpeaking(true);
          hadVoiceRef.current = true;
          console.log('ğŸ—£ï¸ Speaking started!');
        } else if (speakingState && normalizedLevel < LOW) {
          speakingState = false;
          setIsSpeaking(false);
          console.log('ğŸ”‡ Speaking stopped');
        }
        animationFrameRef.current = requestAnimationFrame(updateLevel);
      };
      
      updateLevel();
    } catch (error) {
      console.error('Failed to start audio monitoring:', error);
    }
  };

  const stopAudioMonitoring = () => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }
    setAudioLevel(0);
    setIsSpeaking(false);
  };

  // åˆ‡æ¢è¯­è¨€
  const toggleLanguage = () => {
    setLanguage(prev => prev === 'zh' ? 'en' : 'zh');
  };

  // ä¸»æŒ‰é’®ç‚¹å‡»å¤„ç†
  const handleMainButtonClick = async () => {
    if (state === 'IDLE') {
      // ç¬¬ä¸€æ¬¡ç‚¹å‡»ï¼šå¼€å§‹å½•éŸ³
      startListening();
    } else if (state === 'LISTENING') {
      // ç¬¬äºŒæ¬¡ç‚¹å‡»ï¼šåœæ­¢å½•éŸ³å¹¶å¤„ç†
      stopListeningAndProcess();
    }
  };

  // å¼€å§‹å½•éŸ³
  const startListening = async () => {
    if (!isSpeechRecognitionSupported()) {
      alert('æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½ã€‚\n\nè¯·ä½¿ç”¨ Chrome æˆ– Edge æµè§ˆå™¨ã€‚');
      return;
    }

    // å…ˆè¯·æ±‚éº¦å…‹é£æƒé™ï¼ˆä»…éŸ³é¢‘ï¼‰
    try {
      if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
        await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
      }
      
      setState('LISTENING');
      isRecordingRef.current = true;
      setRealtimeTranscript(''); // æ¸…ç©ºå®æ—¶æ–‡å­—
      
      // å¼€å§‹éŸ³é‡ç›‘æµ‹
      await startAudioMonitoring();

      // å¼€å§‹å½•éŸ³ï¼ˆè¿”å› Promiseï¼Œç”¨äºåœ¨ stop() å‰å·²å¾—åˆ°ç»“æœçš„æƒ…å†µï¼‰
      recognitionRef.current?.start().then((maybeEarlyText) => {
        if (maybeEarlyText && state === 'LISTENING') {
          // æå°‘æ•°æƒ…å†µä¸‹ start å°±è¿”å›äº†æœ€ç»ˆæ–‡æœ¬ï¼ˆæŸäº›å®ç°/æƒé™å¼¹çª—åï¼‰
          setState('PROCESSING');
          isRecordingRef.current = false;
          stopAudioMonitoring();
          processTranscript(maybeEarlyText);
        }
      }).catch((error) => {
        console.error('Speech recognition error:', error);
        alert(`è¯­éŸ³è¯†åˆ«å¯åŠ¨å¤±è´¥ï¼š${error.message}\n\nè¯·æ£€æŸ¥éº¦å…‹é£æƒé™ã€‚`);
        setState('IDLE');
        isRecordingRef.current = false;
        stopAudioMonitoring();
      });
    } catch (error: any) {
      console.error('Microphone permission error:', error);
      alert('éœ€è¦éº¦å…‹é£æƒé™æ‰èƒ½ä½¿ç”¨è¯­éŸ³åŠŸèƒ½ã€‚\n\nè¯·åœ¨æµè§ˆå™¨è®¾ç½®ä¸­å…è®¸éº¦å…‹é£è®¿é—®ã€‚');
      setState('IDLE');
    }
  };

  // åœæ­¢å½•éŸ³å¹¶å¤„ç†
  const stopListeningAndProcess = async () => {
    if (!isRecordingRef.current) return;
    
    setState('PROCESSING');
    isRecordingRef.current = false;
    stopAudioMonitoring(); // åœæ­¢éŸ³é‡ç›‘æµ‹

    let transcript = '';

    if (isSpeechRecognitionSupported() && recognitionRef.current) {
      try {
        // åœæ­¢å½•éŸ³å¹¶è·å–ç»“æœ
        transcript = await recognitionRef.current.stop();
      } catch (error) {
        console.error('Recognition failed:', error);
        alert('è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•ã€‚');
        setState('IDLE');
        setRealtimeTranscript('');
        return;
      }
    }

    // å¦‚æœæ²¡æœ‰è½¬å†™ç»“æœï¼Œå›é€€ä½¿ç”¨å®æ—¶æ–‡å­—
    if (!transcript || !transcript.trim()) {
      if (realtimeTranscript && realtimeTranscript.trim()) {
        transcript = realtimeTranscript.trim();
      } else if (hadVoiceRef.current) {
        // æ›¾æ£€æµ‹åˆ°å£°éŸ³ä½†æœªæ‹¿åˆ°æœ€ç»ˆæ–‡æœ¬ï¼Œä¹Ÿå°è¯•ä½¿ç”¨å®æ—¶æ–‡å­—
        transcript = realtimeTranscript.trim();
      }
      if (!transcript) {
        alert('æ²¡æœ‰æ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡è¯•ã€‚\n\næç¤ºï¼šè¯·ç¡®ä¿åœ¨å®‰é™ç¯å¢ƒä¸‹æ¸…æ™°è¯´è¯ã€‚');
        setState('IDLE');
        setRealtimeTranscript('');
        return;
      }
    }

    console.log('Recognized text:', transcript);

    // è°ƒç”¨ LLM å¤„ç†
    await processTranscript(transcript);
    setRealtimeTranscript(''); // æ¸…ç©ºå®æ—¶æ–‡å­—
  };


  // å¤„ç†è½¬å†™æ–‡æœ¬
  const processTranscript = async (transcript: string) => {
    try {
      // è°ƒç”¨ LLM
      const result: LLMResponse = await callLLM(transcript, language);
      
      // è®¾ç½®å¥å­
      setCurrentSentence({
        zh: result.sentence_zh,
        en: result.sentence_en,
      });
      
      // è¿›å…¥ç¡®è®¤çŠ¶æ€
      setState('CONFIRM');
    } catch (error) {
      console.error('LLM processing error:', error);
      // å‡ºé”™å›åˆ° IDLE
      setState('IDLE');
    }
  };

  // ç¡®è®¤ - Yes
  const handleConfirmYes = async () => {
    setState('OUTPUT');
    
    // TTS æ’­æŠ¥
    const textToSpeak = language === 'zh' ? currentSentence.zh : currentSentence.en;
    await speak(textToSpeak, language);
  };

  // ç¡®è®¤ - No
  const handleConfirmNo = () => {
    // æ˜¾ç¤ºé€‰é¡¹ï¼šå†è¯´ä¸€æ¬¡ æˆ– ä½¿ç”¨å›¾ç‰‡æ¿
    // è¿™é‡Œç®€åŒ–ä¸ºç›´æ¥è¿›å…¥ DISAMBIGUATE
    setState('DISAMBIGUATE');
  };

  // å†è¯´ä¸€æ¬¡
  const handleTryAgain = () => {
    setState('IDLE');
    setCurrentSentence({ zh: '', en: '' });
  };

  // å›¾ç‰‡æ¿é€‰æ‹©
  const handlePictureBoardSelect = (intent: string, sentenceZh: string, sentenceEn: string) => {
    setCurrentSentence({
      zh: sentenceZh,
      en: sentenceEn,
    });
    setState('CONFIRM');
  };

  // å›¾ç‰‡æ¿è¿”å›
  const handlePictureBoardBack = () => {
    setState('IDLE');
  };

  // ä» OUTPUT å›åˆ° IDLE
  const handleRestart = () => {
    setState('IDLE');
    setCurrentSentence({ zh: '', en: '' });
  };

  // æ¸²æŸ“ä¸»æŒ‰é’®æ–‡å­—
  const getMainButtonText = () => {
    switch (state) {
      case 'IDLE':
        return t('speakNow', language);
      case 'LISTENING':
        return t('listening', language);
      case 'PROCESSING':
        return t('processing', language);
      default:
        return '';
    }
  };

  // æ¸²æŸ“ä¸»æ–‡å­—åŒºåŸŸ
  const renderMainText = () => {
    switch (state) {
      case 'IDLE':
        return (
          <div className="main-text">
            <h1 className="welcome-text">{t('welcome', language)}</h1>
            <p className="subtitle-text">{t('subtitle', language)}</p>
          </div>
        );
      
      case 'CONFIRM':
        const sentence = language === 'zh' ? currentSentence.zh : currentSentence.en;
        return (
          <div className="main-text">
            <p className="confirm-text">
              {t('confirmQuestion', language, { sentence })}
            </p>
          </div>
        );
      
      case 'OUTPUT':
        const outputSentence = language === 'zh' ? currentSentence.zh : currentSentence.en;
        return (
          <div className="main-text">
            <p className="output-prefix">{t('outputPrefix', language)}</p>
            <h1 className="output-sentence">{outputSentence}</h1>
          </div>
        );
      
      default:
        return null;
    }
  };

  // æ¸²æŸ“æ“ä½œæŒ‰é’®
  const renderActionButtons = () => {
    if (state === 'CONFIRM') {
      return (
        <div className="action-buttons">
          <button className="action-button action-button-yes" onClick={handleConfirmYes}>
            {t('yes', language)}
          </button>
          <button className="action-button action-button-no" onClick={handleConfirmNo}>
            {t('no', language)}
          </button>
        </div>
      );
    }

    if (state === 'OUTPUT') {
      return (
        <div className="action-buttons">
          <button className="action-button action-button-restart" onClick={handleRestart}>
            {t('tryAgain', language)}
          </button>
        </div>
      );
    }

    return null;
  };

  // DISAMBIGUATE çŠ¶æ€æ˜¾ç¤ºå›¾ç‰‡æ¿
  if (state === 'DISAMBIGUATE') {
    return (
      <PictureBoard
        language={language}
        onSelect={handlePictureBoardSelect}
        onBack={handlePictureBoardBack}
      />
    );
  }

  return (
    <div className="app">
      {/* é¡¶éƒ¨è¯­è¨€åˆ‡æ¢ */}
      <div className="top-bar">
        <button className="language-toggle" onClick={toggleLanguage}>
          {language === 'zh' ? 'EN' : 'ä¸­æ–‡'}
        </button>
      </div>

      {/* ä¸»å†…å®¹åŒº */}
      <div className="main-content">
        {/* è¯­éŸ³çƒ */}
        <VoiceOrb state={state} audioLevel={audioLevel} isSpeaking={isSpeaking} />
        
        {/* å®æ—¶è¯†åˆ«æ–‡å­— */}
        {state === 'LISTENING' && realtimeTranscript && (
          <div className="realtime-transcript">
            <p className="realtime-text">{realtimeTranscript}</p>
          </div>
        )}
        
        {/* ä¸»æ–‡å­—åŒºåŸŸ */}
        {renderMainText()}

        {/* æ“ä½œæŒ‰é’® */}
        {renderActionButtons()}
      </div>

      {/* ä¸»æŒ‰é’® */}
      {(state === 'IDLE' || state === 'LISTENING' || state === 'PROCESSING') && (
        <div className="bottom-bar">
          <button
            className={`main-button ${state.toLowerCase()}`}
            onClick={handleMainButtonClick}
            disabled={state === 'PROCESSING'}
          >
            {getMainButtonText()}
          </button>
        </div>
      )}
    </div>
  );
}

export default App;

